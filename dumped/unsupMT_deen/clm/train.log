INFO - 06/10/22 09:42:38 - 0:00:00 - ============ Initialized logger ============
INFO - 06/10/22 09:42:38 - 0:00:00 - ae_steps: []
                                     asm: False
                                     attention_dropout: 0.1
                                     attention_setting: v1
                                     back_dataset: {}
                                     batch_size: 1
                                     beam_size: 1
                                     bmt_steps: []
                                     bptt: 128
                                     bt_src_langs: ['de', 'en']
                                     bt_steps: [('de', 'en', 'de'), ('en', 'de', 'en')]
                                     clip_grad_norm: 5
                                     clm_steps: [('de', None), ('en', None)]
                                     command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 1 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth' --exp_id "clm"
                                     context_size: 0
                                     cuda: False
                                     data_path: ./data/processed/de-en/
                                     debug_slurm: False
                                     debug_train: False
                                     dropout: 0.1
                                     dump_path: ./dumped/unsupMT_deen\clm
                                     early_stopping: False
                                     emb_dim: 1024
                                     encoder_only: False
                                     english_only: False
                                     epoch_size: 5
                                     eval_bleu: True
                                     eval_only: False
                                     exp_id: clm
                                     exp_name: unsupMT_deen
                                     fp16: False
                                     gelu_activation: True
                                     group_by_size: True
                                     id2lang: {0: 'de', 1: 'en'}
                                     is_master: True
                                     lambda_ae: 1
                                     lambda_bmt: 1
                                     lambda_bt: 1
                                     lambda_clm: 1
                                     lambda_mass: 1
                                     lambda_mlm: 1
                                     lambda_mt: 1
                                     lambda_pc: 1
                                     lambda_span: 10000
                                     lang2id: {'de': 0, 'en': 1}
                                     langs: ['de', 'en']
                                     length_penalty: 1
                                     lg_sampling_factor: -1
                                     lgs: de-en
                                     local_rank: 0
                                     mass_steps: []
                                     master_ip: None
                                     master_port: -1
                                     max_batch_size: 0
                                     max_epoch: 10
                                     max_len: 100
                                     max_vocab: -1
                                     min_count: 0
                                     min_len: 0
                                     mlm_steps: []
                                     mono_dataset: {'de': {'train': './data/processed/de-en/train.de.pth', 'valid': './data/processed/de-en/valid.de.pth', 'test': './data/processed/de-en/test.de.pth'}, 'en': {'train': './data/processed/de-en/train.en.pth', 'valid': './data/processed/de-en/valid.en.pth', 'test': './data/processed/de-en/test.en.pth'}}
                                     mt_steps: []
                                     multi_gpu: False
                                     multi_gpus: False
                                     n_dec_layers: 6
                                     n_gpu_per_node: 1
                                     n_heads: 8
                                     n_langs: 2
                                     n_layers: 6
                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001
                                     para_dataset: {('de', 'en'): {'valid': ('./data/processed/de-en/valid.de-en.de.pth', './data/processed/de-en/valid.de-en.en.pth'), 'test': ('./data/processed/de-en/test.de-en.de.pth', './data/processed/de-en/test.de-en.en.pth')}}
                                     pc_steps: []
                                     prefix: True
                                     reload_model: F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth
                                     sample_alpha: 0
                                     save_periodic: 0
                                     share_inout_emb: True
                                     sinusoidal_embeddings: False
                                     split_data: False
                                     stopping_criterion: _valid_de-en_mt_ppl,3
                                     temperature: None
                                     tokens_per_batch: 2000
                                     validation_metrics: _valid_de-en_mt_ppl
                                     word_blank: 0
                                     word_dropout: 0
                                     word_keep: 0.1
                                     word_mask: 0.8
                                     word_mask_keep_rand: 0.8,0.1,0.1
                                     word_mass: 0
                                     word_pred: 0.15
                                     word_rand: 0.1
                                     word_shuffle: 0
INFO - 06/10/22 09:42:38 - 0:00:00 - The experiment will be stored in ./dumped/unsupMT_deen\clm
                                     
INFO - 06/10/22 09:42:38 - 0:00:00 - Running command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 1 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth'

INFO - 06/10/22 09:42:38 - 0:00:00 - ============ Monolingual data (de)
INFO - 06/10/22 09:42:38 - 0:00:00 - Loading data from ./data/processed/de-en/train.de.pth ...
INFO - 06/10/22 09:42:38 - 0:00:00 - 108350453 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 09:42:39 - 0:00:01 - Removed 0 empty sentences.
INFO - 06/10/22 09:42:39 - 0:00:01 - Removed 6354 too long sentences.

INFO - 06/10/22 09:42:39 - 0:00:01 - Loading data from ./data/processed/de-en/valid.de.pth ...
INFO - 06/10/22 09:42:39 - 0:00:01 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.

INFO - 06/10/22 09:42:40 - 0:00:02 - Loading data from ./data/processed/de-en/test.de.pth ...
INFO - 06/10/22 09:42:40 - 0:00:02 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/10/22 09:42:40 - 0:00:02 - ============ Monolingual data (en)
INFO - 06/10/22 09:42:40 - 0:00:02 - Loading data from ./data/processed/de-en/train.en.pth ...
INFO - 06/10/22 09:42:40 - 0:00:02 - 129883113 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 09:42:41 - 0:00:03 - Removed 0 empty sentences.
INFO - 06/10/22 09:42:42 - 0:00:03 - Removed 13059 too long sentences.

INFO - 06/10/22 09:42:42 - 0:00:04 - Loading data from ./data/processed/de-en/valid.en.pth ...
INFO - 06/10/22 09:42:42 - 0:00:04 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.

INFO - 06/10/22 09:42:42 - 0:00:04 - Loading data from ./data/processed/de-en/test.en.pth ...
INFO - 06/10/22 09:42:42 - 0:00:04 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.


INFO - 06/10/22 09:42:42 - 0:00:04 - ============ Parallel data (de-en)
INFO - 06/10/22 09:42:42 - 0:00:04 - Loading data from ./data/processed/de-en/valid.de-en.de.pth ...
INFO - 06/10/22 09:42:42 - 0:00:04 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.
INFO - 06/10/22 09:42:42 - 0:00:04 - Loading data from ./data/processed/de-en/valid.de-en.en.pth ...
INFO - 06/10/22 09:42:42 - 0:00:04 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.
INFO - 06/10/22 09:42:42 - 0:00:04 - Removed 0 empty sentences.

INFO - 06/10/22 09:42:42 - 0:00:04 - Loading data from ./data/processed/de-en/test.de-en.de.pth ...
INFO - 06/10/22 09:42:42 - 0:00:04 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 09:42:42 - 0:00:04 - Loading data from ./data/processed/de-en/test.de-en.en.pth ...
INFO - 06/10/22 09:42:42 - 0:00:04 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 09:42:43 - 0:00:05 - Removed 0 empty sentences.


INFO - 06/10/22 09:42:43 - 0:00:05 - ============ Data summary
INFO - 06/10/22 09:42:43 - 0:00:05 - Monolingual data   - train -           de:   5000000
INFO - 06/10/22 09:42:43 - 0:00:05 - Monolingual data   - valid -           de:      3000
INFO - 06/10/22 09:42:43 - 0:00:05 - Monolingual data   -  test -           de:      2999
INFO - 06/10/22 09:42:43 - 0:00:05 - Monolingual data   - train -           en:   5000000
INFO - 06/10/22 09:42:43 - 0:00:05 - Monolingual data   - valid -           en:      3000
INFO - 06/10/22 09:42:43 - 0:00:05 - Monolingual data   -  test -           en:      2999
INFO - 06/10/22 09:42:43 - 0:00:05 - Parallel data      - valid -        de-en:      3000
INFO - 06/10/22 09:42:43 - 0:00:05 - Parallel data      -  test -        de-en:      2999

INFO - 06/10/22 09:42:46 - 0:00:08 - Reloading encoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
INFO - 06/10/22 09:42:48 - 0:00:10 - Reloading decoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
DEBUG - 06/10/22 09:42:50 - 0:00:12 - Encoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
DEBUG - 06/10/22 09:42:50 - 0:00:12 - Decoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
INFO - 06/10/22 09:42:50 - 0:00:12 - Number of parameters (encoder): 138023928
INFO - 06/10/22 09:42:50 - 0:00:12 - Number of parameters (decoder): 169524216
INFO - 06/10/22 09:42:56 - 0:00:18 - ============ Starting epoch 0 ... ============
INFO - 06/10/22 09:42:56 - 0:00:18 - Creating new training data iterator (causal,de) ...
INFO - 06/10/22 09:42:56 - 0:00:18 - iterator (causal,de) done
INFO - 06/10/22 09:46:15 - 0:00:00 - ============ Initialized logger ============
INFO - 06/10/22 09:46:15 - 0:00:00 - ae_steps: []
                                     asm: False
                                     attention_dropout: 0.1
                                     attention_setting: v1
                                     back_dataset: {}
                                     batch_size: 2
                                     beam_size: 1
                                     bmt_steps: []
                                     bptt: 128
                                     bt_src_langs: ['de', 'en']
                                     bt_steps: [('de', 'en', 'de'), ('en', 'de', 'en')]
                                     clip_grad_norm: 5
                                     clm_steps: [('de', None), ('en', None)]
                                     command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 2 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth' --exp_id "clm"
                                     context_size: 0
                                     cuda: False
                                     data_path: ./data/processed/de-en/
                                     debug_slurm: False
                                     debug_train: False
                                     dropout: 0.1
                                     dump_path: ./dumped/unsupMT_deen\clm
                                     early_stopping: False
                                     emb_dim: 1024
                                     encoder_only: False
                                     english_only: False
                                     epoch_size: 5
                                     eval_bleu: True
                                     eval_only: False
                                     exp_id: clm
                                     exp_name: unsupMT_deen
                                     fp16: False
                                     gelu_activation: True
                                     group_by_size: True
                                     id2lang: {0: 'de', 1: 'en'}
                                     is_master: True
                                     lambda_ae: 1
                                     lambda_bmt: 1
                                     lambda_bt: 1
                                     lambda_clm: 1
                                     lambda_mass: 1
                                     lambda_mlm: 1
                                     lambda_mt: 1
                                     lambda_pc: 1
                                     lambda_span: 10000
                                     lang2id: {'de': 0, 'en': 1}
                                     langs: ['de', 'en']
                                     length_penalty: 1
                                     lg_sampling_factor: -1
                                     lgs: de-en
                                     local_rank: 0
                                     mass_steps: []
                                     master_ip: None
                                     master_port: -1
                                     max_batch_size: 0
                                     max_epoch: 10
                                     max_len: 100
                                     max_vocab: -1
                                     min_count: 0
                                     min_len: 0
                                     mlm_steps: []
                                     mono_dataset: {'de': {'train': './data/processed/de-en/train.de.pth', 'valid': './data/processed/de-en/valid.de.pth', 'test': './data/processed/de-en/test.de.pth'}, 'en': {'train': './data/processed/de-en/train.en.pth', 'valid': './data/processed/de-en/valid.en.pth', 'test': './data/processed/de-en/test.en.pth'}}
                                     mt_steps: []
                                     multi_gpu: False
                                     multi_gpus: False
                                     n_dec_layers: 6
                                     n_gpu_per_node: 1
                                     n_heads: 8
                                     n_langs: 2
                                     n_layers: 6
                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001
                                     para_dataset: {('de', 'en'): {'valid': ('./data/processed/de-en/valid.de-en.de.pth', './data/processed/de-en/valid.de-en.en.pth'), 'test': ('./data/processed/de-en/test.de-en.de.pth', './data/processed/de-en/test.de-en.en.pth')}}
                                     pc_steps: []
                                     prefix: True
                                     reload_model: F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth
                                     sample_alpha: 0
                                     save_periodic: 0
                                     share_inout_emb: True
                                     sinusoidal_embeddings: False
                                     split_data: False
                                     stopping_criterion: _valid_de-en_mt_ppl,3
                                     temperature: None
                                     tokens_per_batch: 2000
                                     validation_metrics: _valid_de-en_mt_ppl
                                     word_blank: 0
                                     word_dropout: 0
                                     word_keep: 0.1
                                     word_mask: 0.8
                                     word_mask_keep_rand: 0.8,0.1,0.1
                                     word_mass: 0
                                     word_pred: 0.15
                                     word_rand: 0.1
                                     word_shuffle: 0
INFO - 06/10/22 09:46:15 - 0:00:00 - The experiment will be stored in ./dumped/unsupMT_deen\clm
                                     
INFO - 06/10/22 09:46:15 - 0:00:00 - Running command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 2 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth'

INFO - 06/10/22 09:46:15 - 0:00:00 - ============ Monolingual data (de)
INFO - 06/10/22 09:46:15 - 0:00:00 - Loading data from ./data/processed/de-en/train.de.pth ...
INFO - 06/10/22 09:46:15 - 0:00:00 - 108350453 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 09:46:17 - 0:00:02 - Removed 0 empty sentences.
INFO - 06/10/22 09:46:17 - 0:00:02 - Removed 6354 too long sentences.

INFO - 06/10/22 09:46:17 - 0:00:02 - Loading data from ./data/processed/de-en/valid.de.pth ...
INFO - 06/10/22 09:46:17 - 0:00:02 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.

INFO - 06/10/22 09:46:17 - 0:00:02 - Loading data from ./data/processed/de-en/test.de.pth ...
INFO - 06/10/22 09:46:17 - 0:00:02 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/10/22 09:46:17 - 0:00:02 - ============ Monolingual data (en)
INFO - 06/10/22 09:46:17 - 0:00:02 - Loading data from ./data/processed/de-en/train.en.pth ...
INFO - 06/10/22 09:46:18 - 0:00:03 - 129883113 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 09:46:19 - 0:00:04 - Removed 0 empty sentences.
INFO - 06/10/22 09:46:19 - 0:00:05 - Removed 13059 too long sentences.

INFO - 06/10/22 09:46:20 - 0:00:05 - Loading data from ./data/processed/de-en/valid.en.pth ...
INFO - 06/10/22 09:46:20 - 0:00:05 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.

INFO - 06/10/22 09:46:20 - 0:00:05 - Loading data from ./data/processed/de-en/test.en.pth ...
INFO - 06/10/22 09:46:20 - 0:00:05 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.


INFO - 06/10/22 09:46:20 - 0:00:05 - ============ Parallel data (de-en)
INFO - 06/10/22 09:46:20 - 0:00:05 - Loading data from ./data/processed/de-en/valid.de-en.de.pth ...
INFO - 06/10/22 09:46:20 - 0:00:05 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.
INFO - 06/10/22 09:46:20 - 0:00:05 - Loading data from ./data/processed/de-en/valid.de-en.en.pth ...
INFO - 06/10/22 09:46:20 - 0:00:05 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.
INFO - 06/10/22 09:46:20 - 0:00:05 - Removed 0 empty sentences.

INFO - 06/10/22 09:46:20 - 0:00:05 - Loading data from ./data/processed/de-en/test.de-en.de.pth ...
INFO - 06/10/22 09:46:20 - 0:00:05 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 09:46:20 - 0:00:06 - Loading data from ./data/processed/de-en/test.de-en.en.pth ...
INFO - 06/10/22 09:46:21 - 0:00:06 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 09:46:21 - 0:00:06 - Removed 0 empty sentences.


INFO - 06/10/22 09:46:21 - 0:00:06 - ============ Data summary
INFO - 06/10/22 09:46:21 - 0:00:06 - Monolingual data   - train -           de:   5000000
INFO - 06/10/22 09:46:21 - 0:00:06 - Monolingual data   - valid -           de:      3000
INFO - 06/10/22 09:46:21 - 0:00:06 - Monolingual data   -  test -           de:      2999
INFO - 06/10/22 09:46:21 - 0:00:06 - Monolingual data   - train -           en:   5000000
INFO - 06/10/22 09:46:21 - 0:00:06 - Monolingual data   - valid -           en:      3000
INFO - 06/10/22 09:46:21 - 0:00:06 - Monolingual data   -  test -           en:      2999
INFO - 06/10/22 09:46:21 - 0:00:06 - Parallel data      - valid -        de-en:      3000
INFO - 06/10/22 09:46:21 - 0:00:06 - Parallel data      -  test -        de-en:      2999

INFO - 06/10/22 09:46:24 - 0:00:09 - Reloading encoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
INFO - 06/10/22 09:46:26 - 0:00:11 - Reloading decoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
DEBUG - 06/10/22 09:46:28 - 0:00:13 - Encoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
DEBUG - 06/10/22 09:46:28 - 0:00:13 - Decoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
INFO - 06/10/22 09:46:28 - 0:00:13 - Number of parameters (encoder): 138023928
INFO - 06/10/22 09:46:28 - 0:00:13 - Number of parameters (decoder): 169524216
INFO - 06/10/22 09:46:30 - 0:00:15 - ============ Starting epoch 0 ... ============
INFO - 06/10/22 09:46:30 - 0:00:15 - Creating new training data iterator (causal,de) ...
INFO - 06/10/22 09:46:30 - 0:00:15 - iterator (causal,de) done
INFO - 06/10/22 10:54:43 - 0:00:00 - ============ Initialized logger ============
INFO - 06/10/22 10:54:43 - 0:00:00 - ae_steps: []
                                     asm: False
                                     attention_dropout: 0.1
                                     attention_setting: v1
                                     back_dataset: {}
                                     batch_size: 4
                                     beam_size: 1
                                     bmt_steps: []
                                     bptt: 128
                                     bt_src_langs: ['de', 'en']
                                     bt_steps: [('de', 'en', 'de'), ('en', 'de', 'en')]
                                     clip_grad_norm: 5
                                     clm_steps: [('de', None), ('en', None)]
                                     command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 4 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth' --exp_id "clm"
                                     context_size: 0
                                     cuda: False
                                     data_path: ./data/processed/de-en/
                                     debug_slurm: False
                                     debug_train: False
                                     dropout: 0.1
                                     dump_path: ./dumped/unsupMT_deen\clm
                                     early_stopping: False
                                     emb_dim: 1024
                                     encoder_only: False
                                     english_only: False
                                     epoch_size: 5
                                     eval_bleu: True
                                     eval_only: False
                                     exp_id: clm
                                     exp_name: unsupMT_deen
                                     fp16: False
                                     gelu_activation: True
                                     group_by_size: True
                                     id2lang: {0: 'de', 1: 'en'}
                                     is_master: True
                                     lambda_ae: 1
                                     lambda_bmt: 1
                                     lambda_bt: 1
                                     lambda_clm: 1
                                     lambda_mass: 1
                                     lambda_mlm: 1
                                     lambda_mt: 1
                                     lambda_pc: 1
                                     lambda_span: 10000
                                     lang2id: {'de': 0, 'en': 1}
                                     langs: ['de', 'en']
                                     length_penalty: 1
                                     lg_sampling_factor: -1
                                     lgs: de-en
                                     local_rank: 0
                                     mass_steps: []
                                     master_ip: None
                                     master_port: -1
                                     max_batch_size: 0
                                     max_epoch: 10
                                     max_len: 100
                                     max_vocab: -1
                                     min_count: 0
                                     min_len: 0
                                     mlm_steps: []
                                     mono_dataset: {'de': {'train': './data/processed/de-en/train.de.pth', 'valid': './data/processed/de-en/valid.de.pth', 'test': './data/processed/de-en/test.de.pth'}, 'en': {'train': './data/processed/de-en/train.en.pth', 'valid': './data/processed/de-en/valid.en.pth', 'test': './data/processed/de-en/test.en.pth'}}
                                     mt_steps: []
                                     multi_gpu: False
                                     multi_gpus: False
                                     n_dec_layers: 6
                                     n_gpu_per_node: 1
                                     n_heads: 8
                                     n_langs: 2
                                     n_layers: 6
                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001
                                     para_dataset: {('de', 'en'): {'valid': ('./data/processed/de-en/valid.de-en.de.pth', './data/processed/de-en/valid.de-en.en.pth'), 'test': ('./data/processed/de-en/test.de-en.de.pth', './data/processed/de-en/test.de-en.en.pth')}}
                                     pc_steps: []
                                     prefix: True
                                     reload_model: F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth
                                     sample_alpha: 0
                                     save_periodic: 0
                                     share_inout_emb: True
                                     sinusoidal_embeddings: False
                                     split_data: False
                                     stopping_criterion: _valid_de-en_mt_ppl,3
                                     temperature: None
                                     tokens_per_batch: 2000
                                     validation_metrics: _valid_de-en_mt_ppl
                                     word_blank: 0
                                     word_dropout: 0
                                     word_keep: 0.1
                                     word_mask: 0.8
                                     word_mask_keep_rand: 0.8,0.1,0.1
                                     word_mass: 0
                                     word_pred: 0.15
                                     word_rand: 0.1
                                     word_shuffle: 0
INFO - 06/10/22 10:54:43 - 0:00:00 - The experiment will be stored in ./dumped/unsupMT_deen\clm
                                     
INFO - 06/10/22 10:54:43 - 0:00:00 - Running command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 4 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth'

INFO - 06/10/22 10:54:43 - 0:00:00 - ============ Monolingual data (de)
INFO - 06/10/22 10:54:43 - 0:00:00 - Loading data from ./data/processed/de-en/train.de.pth ...
INFO - 06/10/22 10:54:48 - 0:00:05 - 108350453 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 10:54:49 - 0:00:06 - Removed 0 empty sentences.
INFO - 06/10/22 10:54:49 - 0:00:06 - Removed 6354 too long sentences.

INFO - 06/10/22 10:54:49 - 0:00:06 - Loading data from ./data/processed/de-en/valid.de.pth ...
INFO - 06/10/22 10:54:49 - 0:00:06 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.

INFO - 06/10/22 10:54:50 - 0:00:06 - Loading data from ./data/processed/de-en/test.de.pth ...
INFO - 06/10/22 10:54:50 - 0:00:06 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/10/22 10:54:50 - 0:00:07 - ============ Monolingual data (en)
INFO - 06/10/22 10:54:50 - 0:00:07 - Loading data from ./data/processed/de-en/train.en.pth ...
INFO - 06/10/22 10:54:55 - 0:00:12 - 129883113 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 10:54:57 - 0:00:13 - Removed 0 empty sentences.
INFO - 06/10/22 10:54:57 - 0:00:14 - Removed 13059 too long sentences.

INFO - 06/10/22 10:54:57 - 0:00:14 - Loading data from ./data/processed/de-en/valid.en.pth ...
INFO - 06/10/22 10:54:57 - 0:00:14 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.

INFO - 06/10/22 10:54:57 - 0:00:14 - Loading data from ./data/processed/de-en/test.en.pth ...
INFO - 06/10/22 10:54:57 - 0:00:14 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.


INFO - 06/10/22 10:54:58 - 0:00:14 - ============ Parallel data (de-en)
INFO - 06/10/22 10:54:58 - 0:00:14 - Loading data from ./data/processed/de-en/valid.de-en.de.pth ...
INFO - 06/10/22 10:54:58 - 0:00:14 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.
INFO - 06/10/22 10:54:58 - 0:00:14 - Loading data from ./data/processed/de-en/valid.de-en.en.pth ...
INFO - 06/10/22 10:54:58 - 0:00:15 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.
INFO - 06/10/22 10:54:58 - 0:00:15 - Removed 0 empty sentences.

INFO - 06/10/22 10:54:58 - 0:00:15 - Loading data from ./data/processed/de-en/test.de-en.de.pth ...
INFO - 06/10/22 10:54:58 - 0:00:15 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 10:54:58 - 0:00:15 - Loading data from ./data/processed/de-en/test.de-en.en.pth ...
INFO - 06/10/22 10:54:59 - 0:00:15 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 10:54:59 - 0:00:16 - Removed 0 empty sentences.


INFO - 06/10/22 10:54:59 - 0:00:16 - ============ Data summary
INFO - 06/10/22 10:54:59 - 0:00:16 - Monolingual data   - train -           de:   5000000
INFO - 06/10/22 10:54:59 - 0:00:16 - Monolingual data   - valid -           de:      3000
INFO - 06/10/22 10:54:59 - 0:00:16 - Monolingual data   -  test -           de:      2999
INFO - 06/10/22 10:54:59 - 0:00:16 - Monolingual data   - train -           en:   5000000
INFO - 06/10/22 10:54:59 - 0:00:16 - Monolingual data   - valid -           en:      3000
INFO - 06/10/22 10:54:59 - 0:00:16 - Monolingual data   -  test -           en:      2999
INFO - 06/10/22 10:54:59 - 0:00:16 - Parallel data      - valid -        de-en:      3000
INFO - 06/10/22 10:54:59 - 0:00:16 - Parallel data      -  test -        de-en:      2999

INFO - 06/10/22 10:55:02 - 0:00:19 - Reloading encoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
INFO - 06/10/22 10:55:28 - 0:00:45 - Reloading decoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
DEBUG - 06/10/22 10:55:30 - 0:00:47 - Encoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
DEBUG - 06/10/22 10:55:30 - 0:00:47 - Decoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
INFO - 06/10/22 10:55:30 - 0:00:47 - Number of parameters (encoder): 138023928
INFO - 06/10/22 10:55:30 - 0:00:47 - Number of parameters (decoder): 169524216
INFO - 06/10/22 10:55:32 - 0:00:49 - ============ Starting epoch 0 ... ============
INFO - 06/10/22 10:55:32 - 0:00:49 - Creating new training data iterator (causal,de) ...
INFO - 06/10/22 10:55:32 - 0:00:49 - iterator (causal,de) done
INFO - 06/10/22 11:00:06 - 0:00:00 - ============ Initialized logger ============
INFO - 06/10/22 11:00:06 - 0:00:00 - ae_steps: []
                                     asm: False
                                     attention_dropout: 0.1
                                     attention_setting: v1
                                     back_dataset: {}
                                     batch_size: 4
                                     beam_size: 1
                                     bmt_steps: []
                                     bptt: 256
                                     bt_src_langs: ['de', 'en']
                                     bt_steps: [('de', 'en', 'de'), ('en', 'de', 'en')]
                                     clip_grad_norm: 5
                                     clm_steps: [('de', None), ('en', None)]
                                     command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 4 --bptt 256 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth' --exp_id "clm"
                                     context_size: 0
                                     cuda: False
                                     data_path: ./data/processed/de-en/
                                     debug_slurm: False
                                     debug_train: False
                                     dropout: 0.1
                                     dump_path: ./dumped/unsupMT_deen\clm
                                     early_stopping: False
                                     emb_dim: 1024
                                     encoder_only: False
                                     english_only: False
                                     epoch_size: 5
                                     eval_bleu: True
                                     eval_only: False
                                     exp_id: clm
                                     exp_name: unsupMT_deen
                                     fp16: False
                                     gelu_activation: True
                                     group_by_size: True
                                     id2lang: {0: 'de', 1: 'en'}
                                     is_master: True
                                     lambda_ae: 1
                                     lambda_bmt: 1
                                     lambda_bt: 1
                                     lambda_clm: 1
                                     lambda_mass: 1
                                     lambda_mlm: 1
                                     lambda_mt: 1
                                     lambda_pc: 1
                                     lambda_span: 10000
                                     lang2id: {'de': 0, 'en': 1}
                                     langs: ['de', 'en']
                                     length_penalty: 1
                                     lg_sampling_factor: -1
                                     lgs: de-en
                                     local_rank: 0
                                     mass_steps: []
                                     master_ip: None
                                     master_port: -1
                                     max_batch_size: 0
                                     max_epoch: 10
                                     max_len: 100
                                     max_vocab: -1
                                     min_count: 0
                                     min_len: 0
                                     mlm_steps: []
                                     mono_dataset: {'de': {'train': './data/processed/de-en/train.de.pth', 'valid': './data/processed/de-en/valid.de.pth', 'test': './data/processed/de-en/test.de.pth'}, 'en': {'train': './data/processed/de-en/train.en.pth', 'valid': './data/processed/de-en/valid.en.pth', 'test': './data/processed/de-en/test.en.pth'}}
                                     mt_steps: []
                                     multi_gpu: False
                                     multi_gpus: False
                                     n_dec_layers: 6
                                     n_gpu_per_node: 1
                                     n_heads: 8
                                     n_langs: 2
                                     n_layers: 6
                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001
                                     para_dataset: {('de', 'en'): {'valid': ('./data/processed/de-en/valid.de-en.de.pth', './data/processed/de-en/valid.de-en.en.pth'), 'test': ('./data/processed/de-en/test.de-en.de.pth', './data/processed/de-en/test.de-en.en.pth')}}
                                     pc_steps: []
                                     prefix: True
                                     reload_model: F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth
                                     sample_alpha: 0
                                     save_periodic: 0
                                     share_inout_emb: True
                                     sinusoidal_embeddings: False
                                     split_data: False
                                     stopping_criterion: _valid_de-en_mt_ppl,3
                                     temperature: None
                                     tokens_per_batch: 2000
                                     validation_metrics: _valid_de-en_mt_ppl
                                     word_blank: 0
                                     word_dropout: 0
                                     word_keep: 0.1
                                     word_mask: 0.8
                                     word_mask_keep_rand: 0.8,0.1,0.1
                                     word_mass: 0
                                     word_pred: 0.15
                                     word_rand: 0.1
                                     word_shuffle: 0
INFO - 06/10/22 11:00:06 - 0:00:00 - The experiment will be stored in ./dumped/unsupMT_deen\clm
                                     
INFO - 06/10/22 11:00:06 - 0:00:00 - Running command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 4 --bptt 256 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth'

INFO - 06/10/22 11:00:06 - 0:00:00 - ============ Monolingual data (de)
INFO - 06/10/22 11:00:06 - 0:00:00 - Loading data from ./data/processed/de-en/train.de.pth ...
INFO - 06/10/22 11:00:07 - 0:00:00 - 108350453 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 11:00:08 - 0:00:01 - Removed 0 empty sentences.
INFO - 06/10/22 11:00:08 - 0:00:02 - Removed 6354 too long sentences.

INFO - 06/10/22 11:00:08 - 0:00:02 - Loading data from ./data/processed/de-en/valid.de.pth ...
INFO - 06/10/22 11:00:08 - 0:00:02 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.

INFO - 06/10/22 11:00:08 - 0:00:02 - Loading data from ./data/processed/de-en/test.de.pth ...
INFO - 06/10/22 11:00:08 - 0:00:02 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/10/22 11:00:08 - 0:00:02 - ============ Monolingual data (en)
INFO - 06/10/22 11:00:08 - 0:00:02 - Loading data from ./data/processed/de-en/train.en.pth ...
INFO - 06/10/22 11:00:09 - 0:00:02 - 129883113 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 11:00:10 - 0:00:04 - Removed 0 empty sentences.
INFO - 06/10/22 11:00:10 - 0:00:04 - Removed 13059 too long sentences.

INFO - 06/10/22 11:00:10 - 0:00:04 - Loading data from ./data/processed/de-en/valid.en.pth ...
INFO - 06/10/22 11:00:10 - 0:00:04 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.

INFO - 06/10/22 11:00:10 - 0:00:04 - Loading data from ./data/processed/de-en/test.en.pth ...
INFO - 06/10/22 11:00:10 - 0:00:04 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.


INFO - 06/10/22 11:00:10 - 0:00:04 - ============ Parallel data (de-en)
INFO - 06/10/22 11:00:10 - 0:00:04 - Loading data from ./data/processed/de-en/valid.de-en.de.pth ...
INFO - 06/10/22 11:00:10 - 0:00:04 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.
INFO - 06/10/22 11:00:10 - 0:00:04 - Loading data from ./data/processed/de-en/valid.de-en.en.pth ...
INFO - 06/10/22 11:00:10 - 0:00:04 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.
INFO - 06/10/22 11:00:11 - 0:00:05 - Removed 0 empty sentences.

INFO - 06/10/22 11:00:11 - 0:00:05 - Loading data from ./data/processed/de-en/test.de-en.de.pth ...
INFO - 06/10/22 11:00:11 - 0:00:05 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 11:00:11 - 0:00:05 - Loading data from ./data/processed/de-en/test.de-en.en.pth ...
INFO - 06/10/22 11:00:11 - 0:00:05 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 11:00:11 - 0:00:05 - Removed 0 empty sentences.


INFO - 06/10/22 11:00:11 - 0:00:05 - ============ Data summary
INFO - 06/10/22 11:00:11 - 0:00:05 - Monolingual data   - train -           de:   5000000
INFO - 06/10/22 11:00:11 - 0:00:05 - Monolingual data   - valid -           de:      3000
INFO - 06/10/22 11:00:11 - 0:00:05 - Monolingual data   -  test -           de:      2999
INFO - 06/10/22 11:00:11 - 0:00:05 - Monolingual data   - train -           en:   5000000
INFO - 06/10/22 11:00:11 - 0:00:05 - Monolingual data   - valid -           en:      3000
INFO - 06/10/22 11:00:11 - 0:00:05 - Monolingual data   -  test -           en:      2999
INFO - 06/10/22 11:00:11 - 0:00:05 - Parallel data      - valid -        de-en:      3000
INFO - 06/10/22 11:00:11 - 0:00:05 - Parallel data      -  test -        de-en:      2999

INFO - 06/10/22 11:00:14 - 0:00:08 - Reloading encoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
INFO - 06/10/22 11:00:17 - 0:00:11 - Reloading decoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
DEBUG - 06/10/22 11:00:18 - 0:00:12 - Encoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
DEBUG - 06/10/22 11:00:18 - 0:00:12 - Decoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
INFO - 06/10/22 11:00:18 - 0:00:12 - Number of parameters (encoder): 138023928
INFO - 06/10/22 11:00:18 - 0:00:12 - Number of parameters (decoder): 169524216
INFO - 06/10/22 11:00:20 - 0:00:14 - ============ Starting epoch 0 ... ============
INFO - 06/10/22 11:00:20 - 0:00:14 - Creating new training data iterator (causal,en) ...
INFO - 06/10/22 11:00:20 - 0:00:14 - iterator (causal,en) done
INFO - 06/10/22 11:01:20 - 0:00:00 - ============ Initialized logger ============
INFO - 06/10/22 11:01:20 - 0:00:00 - ae_steps: []
                                     asm: False
                                     attention_dropout: 0.1
                                     attention_setting: v1
                                     back_dataset: {}
                                     batch_size: 4
                                     beam_size: 1
                                     bmt_steps: []
                                     bptt: 512
                                     bt_src_langs: ['de', 'en']
                                     bt_steps: [('de', 'en', 'de'), ('en', 'de', 'en')]
                                     clip_grad_norm: 5
                                     clm_steps: [('de', None), ('en', None)]
                                     command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 4 --bptt 512 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth' --exp_id "clm"
                                     context_size: 0
                                     cuda: False
                                     data_path: ./data/processed/de-en/
                                     debug_slurm: False
                                     debug_train: False
                                     dropout: 0.1
                                     dump_path: ./dumped/unsupMT_deen\clm
                                     early_stopping: False
                                     emb_dim: 1024
                                     encoder_only: False
                                     english_only: False
                                     epoch_size: 5
                                     eval_bleu: True
                                     eval_only: False
                                     exp_id: clm
                                     exp_name: unsupMT_deen
                                     fp16: False
                                     gelu_activation: True
                                     group_by_size: True
                                     id2lang: {0: 'de', 1: 'en'}
                                     is_master: True
                                     lambda_ae: 1
                                     lambda_bmt: 1
                                     lambda_bt: 1
                                     lambda_clm: 1
                                     lambda_mass: 1
                                     lambda_mlm: 1
                                     lambda_mt: 1
                                     lambda_pc: 1
                                     lambda_span: 10000
                                     lang2id: {'de': 0, 'en': 1}
                                     langs: ['de', 'en']
                                     length_penalty: 1
                                     lg_sampling_factor: -1
                                     lgs: de-en
                                     local_rank: 0
                                     mass_steps: []
                                     master_ip: None
                                     master_port: -1
                                     max_batch_size: 0
                                     max_epoch: 10
                                     max_len: 100
                                     max_vocab: -1
                                     min_count: 0
                                     min_len: 0
                                     mlm_steps: []
                                     mono_dataset: {'de': {'train': './data/processed/de-en/train.de.pth', 'valid': './data/processed/de-en/valid.de.pth', 'test': './data/processed/de-en/test.de.pth'}, 'en': {'train': './data/processed/de-en/train.en.pth', 'valid': './data/processed/de-en/valid.en.pth', 'test': './data/processed/de-en/test.en.pth'}}
                                     mt_steps: []
                                     multi_gpu: False
                                     multi_gpus: False
                                     n_dec_layers: 6
                                     n_gpu_per_node: 1
                                     n_heads: 8
                                     n_langs: 2
                                     n_layers: 6
                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001
                                     para_dataset: {('de', 'en'): {'valid': ('./data/processed/de-en/valid.de-en.de.pth', './data/processed/de-en/valid.de-en.en.pth'), 'test': ('./data/processed/de-en/test.de-en.de.pth', './data/processed/de-en/test.de-en.en.pth')}}
                                     pc_steps: []
                                     prefix: True
                                     reload_model: F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth
                                     sample_alpha: 0
                                     save_periodic: 0
                                     share_inout_emb: True
                                     sinusoidal_embeddings: False
                                     split_data: False
                                     stopping_criterion: _valid_de-en_mt_ppl,3
                                     temperature: None
                                     tokens_per_batch: 2000
                                     validation_metrics: _valid_de-en_mt_ppl
                                     word_blank: 0
                                     word_dropout: 0
                                     word_keep: 0.1
                                     word_mask: 0.8
                                     word_mask_keep_rand: 0.8,0.1,0.1
                                     word_mass: 0
                                     word_pred: 0.15
                                     word_rand: 0.1
                                     word_shuffle: 0
INFO - 06/10/22 11:01:20 - 0:00:00 - The experiment will be stored in ./dumped/unsupMT_deen\clm
                                     
INFO - 06/10/22 11:01:20 - 0:00:00 - Running command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 4 --bptt 512 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth'

INFO - 06/10/22 11:01:20 - 0:00:00 - ============ Monolingual data (de)
INFO - 06/10/22 11:01:20 - 0:00:00 - Loading data from ./data/processed/de-en/train.de.pth ...
INFO - 06/10/22 11:01:20 - 0:00:00 - 108350453 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 11:01:21 - 0:00:01 - Removed 0 empty sentences.
INFO - 06/10/22 11:01:21 - 0:00:01 - Removed 6354 too long sentences.

INFO - 06/10/22 11:01:21 - 0:00:01 - Loading data from ./data/processed/de-en/valid.de.pth ...
INFO - 06/10/22 11:01:22 - 0:00:01 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.

INFO - 06/10/22 11:01:22 - 0:00:02 - Loading data from ./data/processed/de-en/test.de.pth ...
INFO - 06/10/22 11:01:22 - 0:00:02 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/10/22 11:01:22 - 0:00:02 - ============ Monolingual data (en)
INFO - 06/10/22 11:01:22 - 0:00:02 - Loading data from ./data/processed/de-en/train.en.pth ...
INFO - 06/10/22 11:01:22 - 0:00:02 - 129883113 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 11:01:24 - 0:00:04 - Removed 0 empty sentences.
INFO - 06/10/22 11:01:24 - 0:00:04 - Removed 13059 too long sentences.

INFO - 06/10/22 11:01:24 - 0:00:04 - Loading data from ./data/processed/de-en/valid.en.pth ...
INFO - 06/10/22 11:01:24 - 0:00:04 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.

INFO - 06/10/22 11:01:24 - 0:00:04 - Loading data from ./data/processed/de-en/test.en.pth ...
INFO - 06/10/22 11:01:24 - 0:00:04 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.


INFO - 06/10/22 11:01:24 - 0:00:04 - ============ Parallel data (de-en)
INFO - 06/10/22 11:01:24 - 0:00:04 - Loading data from ./data/processed/de-en/valid.de-en.de.pth ...
INFO - 06/10/22 11:01:24 - 0:00:04 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.
INFO - 06/10/22 11:01:24 - 0:00:04 - Loading data from ./data/processed/de-en/valid.de-en.en.pth ...
INFO - 06/10/22 11:01:24 - 0:00:04 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.
INFO - 06/10/22 11:01:25 - 0:00:05 - Removed 0 empty sentences.

INFO - 06/10/22 11:01:25 - 0:00:05 - Loading data from ./data/processed/de-en/test.de-en.de.pth ...
INFO - 06/10/22 11:01:25 - 0:00:05 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 11:01:25 - 0:00:05 - Loading data from ./data/processed/de-en/test.de-en.en.pth ...
INFO - 06/10/22 11:01:25 - 0:00:05 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 11:01:25 - 0:00:05 - Removed 0 empty sentences.


INFO - 06/10/22 11:01:25 - 0:00:05 - ============ Data summary
INFO - 06/10/22 11:01:25 - 0:00:05 - Monolingual data   - train -           de:   5000000
INFO - 06/10/22 11:01:25 - 0:00:05 - Monolingual data   - valid -           de:      3000
INFO - 06/10/22 11:01:25 - 0:00:05 - Monolingual data   -  test -           de:      2999
INFO - 06/10/22 11:01:25 - 0:00:05 - Monolingual data   - train -           en:   5000000
INFO - 06/10/22 11:01:25 - 0:00:05 - Monolingual data   - valid -           en:      3000
INFO - 06/10/22 11:01:25 - 0:00:05 - Monolingual data   -  test -           en:      2999
INFO - 06/10/22 11:01:25 - 0:00:05 - Parallel data      - valid -        de-en:      3000
INFO - 06/10/22 11:01:25 - 0:00:05 - Parallel data      -  test -        de-en:      2999

INFO - 06/10/22 11:01:28 - 0:00:08 - Reloading encoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
INFO - 06/10/22 11:01:31 - 0:00:11 - Reloading decoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
DEBUG - 06/10/22 11:01:32 - 0:00:12 - Encoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
DEBUG - 06/10/22 11:01:32 - 0:00:12 - Decoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
INFO - 06/10/22 11:01:32 - 0:00:12 - Number of parameters (encoder): 138023928
INFO - 06/10/22 11:01:32 - 0:00:12 - Number of parameters (decoder): 169524216
INFO - 06/10/22 11:01:35 - 0:00:14 - ============ Starting epoch 0 ... ============
INFO - 06/10/22 11:01:35 - 0:00:14 - Creating new training data iterator (causal,de) ...
INFO - 06/10/22 11:01:35 - 0:00:14 - iterator (causal,de) done
INFO - 06/10/22 11:23:20 - 0:00:00 - ============ Initialized logger ============
INFO - 06/10/22 11:23:20 - 0:00:00 - ae_steps: []
                                     asm: False
                                     attention_dropout: 0.1
                                     attention_setting: v1
                                     back_dataset: {}
                                     batch_size: 4
                                     beam_size: 1
                                     bmt_steps: []
                                     bptt: 512
                                     bt_src_langs: ['de', 'en']
                                     bt_steps: [('de', 'en', 'de'), ('en', 'de', 'en')]
                                     clip_grad_norm: 5
                                     clm_steps: [('de', None), ('en', None)]
                                     command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 4 --bptt 512 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth' --exp_id "clm"
                                     context_size: 0
                                     cuda: False
                                     data_path: ./data/processed/de-en/
                                     debug_slurm: False
                                     debug_train: False
                                     dropout: 0.1
                                     dump_path: ./dumped/unsupMT_deen\clm
                                     early_stopping: False
                                     emb_dim: 1024
                                     encoder_only: False
                                     english_only: False
                                     epoch_size: 5
                                     eval_bleu: True
                                     eval_only: False
                                     exp_id: clm
                                     exp_name: unsupMT_deen
                                     fp16: False
                                     gelu_activation: True
                                     group_by_size: True
                                     id2lang: {0: 'de', 1: 'en'}
                                     is_master: True
                                     lambda_ae: 1
                                     lambda_bmt: 1
                                     lambda_bt: 1
                                     lambda_clm: 1
                                     lambda_mass: 1
                                     lambda_mlm: 1
                                     lambda_mt: 1
                                     lambda_pc: 1
                                     lambda_span: 10000
                                     lang2id: {'de': 0, 'en': 1}
                                     langs: ['de', 'en']
                                     length_penalty: 1
                                     lg_sampling_factor: -1
                                     lgs: de-en
                                     local_rank: 0
                                     mass_steps: []
                                     master_ip: None
                                     master_port: -1
                                     max_batch_size: 0
                                     max_epoch: 10
                                     max_len: 100
                                     max_vocab: -1
                                     min_count: 0
                                     min_len: 0
                                     mlm_steps: []
                                     mono_dataset: {'de': {'train': './data/processed/de-en/train.de.pth', 'valid': './data/processed/de-en/valid.de.pth', 'test': './data/processed/de-en/test.de.pth'}, 'en': {'train': './data/processed/de-en/train.en.pth', 'valid': './data/processed/de-en/valid.en.pth', 'test': './data/processed/de-en/test.en.pth'}}
                                     mt_steps: []
                                     multi_gpu: False
                                     multi_gpus: False
                                     n_dec_layers: 6
                                     n_gpu_per_node: 1
                                     n_heads: 8
                                     n_langs: 2
                                     n_layers: 6
                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001
                                     para_dataset: {('de', 'en'): {'valid': ('./data/processed/de-en/valid.de-en.de.pth', './data/processed/de-en/valid.de-en.en.pth'), 'test': ('./data/processed/de-en/test.de-en.de.pth', './data/processed/de-en/test.de-en.en.pth')}}
                                     pc_steps: []
                                     prefix: True
                                     reload_model: F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth
                                     sample_alpha: 0
                                     save_periodic: 0
                                     share_inout_emb: True
                                     sinusoidal_embeddings: False
                                     split_data: False
                                     stopping_criterion: _valid_de-en_mt_ppl,3
                                     temperature: None
                                     tokens_per_batch: 2000
                                     validation_metrics: _valid_de-en_mt_ppl
                                     word_blank: 0
                                     word_dropout: 0
                                     word_keep: 0.1
                                     word_mask: 0.8
                                     word_mask_keep_rand: 0.8,0.1,0.1
                                     word_mass: 0
                                     word_pred: 0.15
                                     word_rand: 0.1
                                     word_shuffle: 0
INFO - 06/10/22 11:23:20 - 0:00:00 - The experiment will be stored in ./dumped/unsupMT_deen\clm
                                     
INFO - 06/10/22 11:23:20 - 0:00:00 - Running command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 4 --bptt 512 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth'

INFO - 06/10/22 11:23:20 - 0:00:00 - ============ Monolingual data (de)
INFO - 06/10/22 11:23:20 - 0:00:00 - Loading data from ./data/processed/de-en/train.de.pth ...
INFO - 06/10/22 11:23:20 - 0:00:00 - 108350453 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 11:23:21 - 0:00:01 - Removed 0 empty sentences.
INFO - 06/10/22 11:23:21 - 0:00:01 - Removed 6354 too long sentences.

INFO - 06/10/22 11:23:21 - 0:00:02 - Loading data from ./data/processed/de-en/valid.de.pth ...
INFO - 06/10/22 11:23:21 - 0:00:02 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.

INFO - 06/10/22 11:23:22 - 0:00:02 - Loading data from ./data/processed/de-en/test.de.pth ...
INFO - 06/10/22 11:23:22 - 0:00:02 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/10/22 11:23:22 - 0:00:02 - ============ Monolingual data (en)
INFO - 06/10/22 11:23:22 - 0:00:02 - Loading data from ./data/processed/de-en/train.en.pth ...
INFO - 06/10/22 11:23:22 - 0:00:02 - 129883113 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 11:23:24 - 0:00:04 - Removed 0 empty sentences.
INFO - 06/10/22 11:23:24 - 0:00:04 - Removed 13059 too long sentences.

INFO - 06/10/22 11:23:24 - 0:00:04 - Loading data from ./data/processed/de-en/valid.en.pth ...
INFO - 06/10/22 11:23:24 - 0:00:04 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.

INFO - 06/10/22 11:23:24 - 0:00:04 - Loading data from ./data/processed/de-en/test.en.pth ...
INFO - 06/10/22 11:23:24 - 0:00:04 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.


INFO - 06/10/22 11:23:24 - 0:00:04 - ============ Parallel data (de-en)
INFO - 06/10/22 11:23:24 - 0:00:04 - Loading data from ./data/processed/de-en/valid.de-en.de.pth ...
INFO - 06/10/22 11:23:24 - 0:00:04 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.
INFO - 06/10/22 11:23:24 - 0:00:04 - Loading data from ./data/processed/de-en/valid.de-en.en.pth ...
INFO - 06/10/22 11:23:24 - 0:00:04 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.
INFO - 06/10/22 11:23:25 - 0:00:05 - Removed 0 empty sentences.

INFO - 06/10/22 11:23:25 - 0:00:05 - Loading data from ./data/processed/de-en/test.de-en.de.pth ...
INFO - 06/10/22 11:23:25 - 0:00:05 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 11:23:25 - 0:00:05 - Loading data from ./data/processed/de-en/test.de-en.en.pth ...
INFO - 06/10/22 11:23:25 - 0:00:05 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 11:23:25 - 0:00:05 - Removed 0 empty sentences.


INFO - 06/10/22 11:23:25 - 0:00:05 - ============ Data summary
INFO - 06/10/22 11:23:25 - 0:00:05 - Monolingual data   - train -           de:   5000000
INFO - 06/10/22 11:23:25 - 0:00:05 - Monolingual data   - valid -           de:      3000
INFO - 06/10/22 11:23:25 - 0:00:05 - Monolingual data   -  test -           de:      2999
INFO - 06/10/22 11:23:25 - 0:00:05 - Monolingual data   - train -           en:   5000000
INFO - 06/10/22 11:23:25 - 0:00:05 - Monolingual data   - valid -           en:      3000
INFO - 06/10/22 11:23:25 - 0:00:05 - Monolingual data   -  test -           en:      2999
INFO - 06/10/22 11:23:25 - 0:00:05 - Parallel data      - valid -        de-en:      3000
INFO - 06/10/22 11:23:25 - 0:00:05 - Parallel data      -  test -        de-en:      2999

INFO - 06/10/22 11:23:28 - 0:00:08 - Reloading encoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
INFO - 06/10/22 11:23:31 - 0:00:11 - Reloading decoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
DEBUG - 06/10/22 11:23:32 - 0:00:12 - Encoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
DEBUG - 06/10/22 11:23:32 - 0:00:12 - Decoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
INFO - 06/10/22 11:23:32 - 0:00:12 - Number of parameters (encoder): 138023928
INFO - 06/10/22 11:23:32 - 0:00:12 - Number of parameters (decoder): 169524216
INFO - 06/10/22 11:23:34 - 0:00:15 - ============ Starting epoch 0 ... ============
INFO - 06/10/22 11:23:34 - 0:00:15 - Creating new training data iterator (causal,en) ...
INFO - 06/10/22 11:23:34 - 0:00:15 - iterator (causal,en) done
INFO - 06/10/22 12:35:58 - 0:00:00 - ============ Initialized logger ============
INFO - 06/10/22 12:35:58 - 0:00:00 - ae_steps: []
                                     asm: False
                                     attention_dropout: 0.1
                                     attention_setting: v1
                                     back_dataset: {}
                                     batch_size: 2
                                     beam_size: 1
                                     bmt_steps: []
                                     bptt: 128
                                     bt_src_langs: []
                                     bt_steps: []
                                     clip_grad_norm: 5
                                     clm_steps: [('de', None), ('en', None)]
                                     command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 2 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu false --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth' --exp_id "clm"
                                     context_size: 0
                                     cuda: False
                                     data_path: ./data/processed/de-en/
                                     debug_slurm: False
                                     debug_train: False
                                     dropout: 0.1
                                     dump_path: ./dumped/unsupMT_deen\clm
                                     early_stopping: False
                                     emb_dim: 1024
                                     encoder_only: False
                                     english_only: False
                                     epoch_size: 5
                                     eval_bleu: False
                                     eval_only: False
                                     exp_id: clm
                                     exp_name: unsupMT_deen
                                     fp16: False
                                     gelu_activation: True
                                     group_by_size: True
                                     id2lang: {0: 'de', 1: 'en'}
                                     is_master: True
                                     lambda_ae: 1
                                     lambda_bmt: 1
                                     lambda_bt: 1
                                     lambda_clm: 1
                                     lambda_mass: 1
                                     lambda_mlm: 1
                                     lambda_mt: 1
                                     lambda_pc: 1
                                     lambda_span: 10000
                                     lang2id: {'de': 0, 'en': 1}
                                     langs: ['de', 'en']
                                     length_penalty: 1
                                     lg_sampling_factor: -1
                                     lgs: de-en
                                     local_rank: 0
                                     mass_steps: []
                                     master_ip: None
                                     master_port: -1
                                     max_batch_size: 0
                                     max_epoch: 10
                                     max_len: 100
                                     max_vocab: -1
                                     min_count: 0
                                     min_len: 0
                                     mlm_steps: []
                                     mono_dataset: {'de': {'train': './data/processed/de-en/train.de.pth', 'valid': './data/processed/de-en/valid.de.pth', 'test': './data/processed/de-en/test.de.pth'}, 'en': {'train': './data/processed/de-en/train.en.pth', 'valid': './data/processed/de-en/valid.en.pth', 'test': './data/processed/de-en/test.en.pth'}}
                                     mt_steps: []
                                     multi_gpu: False
                                     multi_gpus: False
                                     n_dec_layers: 6
                                     n_gpu_per_node: 1
                                     n_heads: 8
                                     n_langs: 2
                                     n_layers: 6
                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001
                                     para_dataset: {}
                                     pc_steps: []
                                     prefix: True
                                     reload_model: F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth
                                     sample_alpha: 0
                                     save_periodic: 0
                                     share_inout_emb: True
                                     sinusoidal_embeddings: False
                                     split_data: False
                                     stopping_criterion: _valid_de-en_mt_ppl,3
                                     temperature: None
                                     tokens_per_batch: 2000
                                     validation_metrics: _valid_de-en_mt_ppl
                                     word_blank: 0
                                     word_dropout: 0
                                     word_keep: 0.1
                                     word_mask: 0.8
                                     word_mask_keep_rand: 0.8,0.1,0.1
                                     word_mass: 0
                                     word_pred: 0.15
                                     word_rand: 0.1
                                     word_shuffle: 0
INFO - 06/10/22 12:35:58 - 0:00:00 - The experiment will be stored in ./dumped/unsupMT_deen\clm
                                     
INFO - 06/10/22 12:35:58 - 0:00:00 - Running command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 2 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu false --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth'

INFO - 06/10/22 12:35:58 - 0:00:00 - ============ Monolingual data (de)
INFO - 06/10/22 12:35:58 - 0:00:00 - Loading data from ./data/processed/de-en/train.de.pth ...
INFO - 06/10/22 12:35:59 - 0:00:01 - 108350453 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/10/22 12:36:00 - 0:00:01 - Loading data from ./data/processed/de-en/valid.de.pth ...
INFO - 06/10/22 12:36:00 - 0:00:01 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.

INFO - 06/10/22 12:36:00 - 0:00:02 - Loading data from ./data/processed/de-en/test.de.pth ...
INFO - 06/10/22 12:36:00 - 0:00:02 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/10/22 12:36:00 - 0:00:02 - ============ Monolingual data (en)
INFO - 06/10/22 12:36:00 - 0:00:02 - Loading data from ./data/processed/de-en/train.en.pth ...
INFO - 06/10/22 12:36:01 - 0:00:02 - 129883113 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/10/22 12:36:02 - 0:00:03 - Loading data from ./data/processed/de-en/valid.en.pth ...
INFO - 06/10/22 12:36:02 - 0:00:03 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.

INFO - 06/10/22 12:36:02 - 0:00:04 - Loading data from ./data/processed/de-en/test.en.pth ...
INFO - 06/10/22 12:36:02 - 0:00:04 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.



INFO - 06/10/22 12:36:02 - 0:00:04 - ============ Data summary
INFO - 06/10/22 12:36:02 - 0:00:04 - Monolingual data   - train -           de:   5000000
INFO - 06/10/22 12:36:02 - 0:00:04 - Monolingual data   - valid -           de:      3000
INFO - 06/10/22 12:36:02 - 0:00:04 - Monolingual data   -  test -           de:      2999
INFO - 06/10/22 12:36:02 - 0:00:04 - Monolingual data   - train -           en:   5000000
INFO - 06/10/22 12:36:02 - 0:00:04 - Monolingual data   - valid -           en:      3000
INFO - 06/10/22 12:36:02 - 0:00:04 - Monolingual data   -  test -           en:      2999

INFO - 06/10/22 12:36:05 - 0:00:07 - Reloading encoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
INFO - 06/10/22 12:36:08 - 0:00:10 - Reloading decoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
DEBUG - 06/10/22 12:36:10 - 0:00:12 - Encoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
DEBUG - 06/10/22 12:36:10 - 0:00:12 - Decoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
INFO - 06/10/22 12:36:10 - 0:00:12 - Number of parameters (encoder): 138023928
INFO - 06/10/22 12:36:10 - 0:00:12 - Number of parameters (decoder): 169524216
INFO - 06/10/22 12:36:11 - 0:00:12 - ============ Starting epoch 0 ... ============
INFO - 06/10/22 12:36:11 - 0:00:12 - Creating new training data iterator (causal,en) ...
INFO - 06/10/22 12:36:11 - 0:00:12 - iterator (causal,en) done
INFO - 06/10/22 12:37:04 - 0:00:00 - ============ Initialized logger ============
INFO - 06/10/22 12:37:04 - 0:00:00 - ae_steps: []
                                     asm: False
                                     attention_dropout: 0.1
                                     attention_setting: v1
                                     back_dataset: {}
                                     batch_size: 2
                                     beam_size: 1
                                     bmt_steps: []
                                     bptt: 128
                                     bt_src_langs: ['de', 'en']
                                     bt_steps: [('de', 'en', 'de'), ('en', 'de', 'en')]
                                     clip_grad_norm: 5
                                     clm_steps: []
                                     command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 2 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth' --exp_id "clm"
                                     context_size: 0
                                     cuda: False
                                     data_path: ./data/processed/de-en/
                                     debug_slurm: False
                                     debug_train: False
                                     dropout: 0.1
                                     dump_path: ./dumped/unsupMT_deen\clm
                                     early_stopping: False
                                     emb_dim: 1024
                                     encoder_only: False
                                     english_only: False
                                     epoch_size: 5
                                     eval_bleu: True
                                     eval_only: False
                                     exp_id: clm
                                     exp_name: unsupMT_deen
                                     fp16: False
                                     gelu_activation: True
                                     group_by_size: True
                                     id2lang: {0: 'de', 1: 'en'}
                                     is_master: True
                                     lambda_ae: 1
                                     lambda_bmt: 1
                                     lambda_bt: 1
                                     lambda_clm: 1
                                     lambda_mass: 1
                                     lambda_mlm: 1
                                     lambda_mt: 1
                                     lambda_pc: 1
                                     lambda_span: 10000
                                     lang2id: {'de': 0, 'en': 1}
                                     langs: ['de', 'en']
                                     length_penalty: 1
                                     lg_sampling_factor: -1
                                     lgs: de-en
                                     local_rank: 0
                                     mass_steps: []
                                     master_ip: None
                                     master_port: -1
                                     max_batch_size: 0
                                     max_epoch: 10
                                     max_len: 100
                                     max_vocab: -1
                                     min_count: 0
                                     min_len: 0
                                     mlm_steps: []
                                     mono_dataset: {'de': {'train': './data/processed/de-en/train.de.pth', 'valid': './data/processed/de-en/valid.de.pth', 'test': './data/processed/de-en/test.de.pth'}, 'en': {'train': './data/processed/de-en/train.en.pth', 'valid': './data/processed/de-en/valid.en.pth', 'test': './data/processed/de-en/test.en.pth'}}
                                     mt_steps: []
                                     multi_gpu: False
                                     multi_gpus: False
                                     n_dec_layers: 6
                                     n_gpu_per_node: 1
                                     n_heads: 8
                                     n_langs: 2
                                     n_layers: 6
                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001
                                     para_dataset: {('de', 'en'): {'valid': ('./data/processed/de-en/valid.de-en.de.pth', './data/processed/de-en/valid.de-en.en.pth'), 'test': ('./data/processed/de-en/test.de-en.de.pth', './data/processed/de-en/test.de-en.en.pth')}}
                                     pc_steps: []
                                     prefix: True
                                     reload_model: F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth
                                     sample_alpha: 0
                                     save_periodic: 0
                                     share_inout_emb: True
                                     sinusoidal_embeddings: False
                                     split_data: False
                                     stopping_criterion: _valid_de-en_mt_ppl,3
                                     temperature: None
                                     tokens_per_batch: 2000
                                     validation_metrics: _valid_de-en_mt_ppl
                                     word_blank: 0
                                     word_dropout: 0
                                     word_keep: 0.1
                                     word_mask: 0.8
                                     word_mask_keep_rand: 0.8,0.1,0.1
                                     word_mass: 0
                                     word_pred: 0.15
                                     word_rand: 0.1
                                     word_shuffle: 0
INFO - 06/10/22 12:37:04 - 0:00:00 - The experiment will be stored in ./dumped/unsupMT_deen\clm
                                     
INFO - 06/10/22 12:37:04 - 0:00:00 - Running command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 2 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth'

INFO - 06/10/22 12:37:04 - 0:00:00 - ============ Monolingual data (de)
INFO - 06/10/22 12:37:04 - 0:00:00 - Loading data from ./data/processed/de-en/train.de.pth ...
INFO - 06/10/22 12:37:04 - 0:00:00 - 108350453 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 12:37:05 - 0:00:02 - Removed 0 empty sentences.
INFO - 06/10/22 12:37:06 - 0:00:02 - Removed 6354 too long sentences.

INFO - 06/10/22 12:37:06 - 0:00:02 - Loading data from ./data/processed/de-en/valid.de.pth ...
INFO - 06/10/22 12:37:06 - 0:00:02 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.

INFO - 06/10/22 12:37:06 - 0:00:02 - Loading data from ./data/processed/de-en/test.de.pth ...
INFO - 06/10/22 12:37:06 - 0:00:02 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/10/22 12:37:06 - 0:00:02 - ============ Monolingual data (en)
INFO - 06/10/22 12:37:06 - 0:00:02 - Loading data from ./data/processed/de-en/train.en.pth ...
INFO - 06/10/22 12:37:07 - 0:00:03 - 129883113 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 12:37:08 - 0:00:04 - Removed 0 empty sentences.
INFO - 06/10/22 12:37:08 - 0:00:05 - Removed 13059 too long sentences.

INFO - 06/10/22 12:37:08 - 0:00:05 - Loading data from ./data/processed/de-en/valid.en.pth ...
INFO - 06/10/22 12:37:08 - 0:00:05 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.

INFO - 06/10/22 12:37:09 - 0:00:05 - Loading data from ./data/processed/de-en/test.en.pth ...
INFO - 06/10/22 12:37:09 - 0:00:05 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.


INFO - 06/10/22 12:37:09 - 0:00:05 - ============ Parallel data (de-en)
INFO - 06/10/22 12:37:09 - 0:00:05 - Loading data from ./data/processed/de-en/valid.de-en.de.pth ...
INFO - 06/10/22 12:37:09 - 0:00:05 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.
INFO - 06/10/22 12:37:09 - 0:00:05 - Loading data from ./data/processed/de-en/valid.de-en.en.pth ...
INFO - 06/10/22 12:37:09 - 0:00:05 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.
INFO - 06/10/22 12:37:09 - 0:00:06 - Removed 0 empty sentences.

INFO - 06/10/22 12:37:09 - 0:00:06 - Loading data from ./data/processed/de-en/test.de-en.de.pth ...
INFO - 06/10/22 12:37:09 - 0:00:06 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 12:37:09 - 0:00:06 - Loading data from ./data/processed/de-en/test.de-en.en.pth ...
INFO - 06/10/22 12:37:09 - 0:00:06 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 12:37:10 - 0:00:06 - Removed 0 empty sentences.


INFO - 06/10/22 12:37:10 - 0:00:06 - ============ Data summary
INFO - 06/10/22 12:37:10 - 0:00:06 - Monolingual data   - train -           de:   5000000
INFO - 06/10/22 12:37:10 - 0:00:06 - Monolingual data   - valid -           de:      3000
INFO - 06/10/22 12:37:10 - 0:00:06 - Monolingual data   -  test -           de:      2999
INFO - 06/10/22 12:37:10 - 0:00:06 - Monolingual data   - train -           en:   5000000
INFO - 06/10/22 12:37:10 - 0:00:06 - Monolingual data   - valid -           en:      3000
INFO - 06/10/22 12:37:10 - 0:00:06 - Monolingual data   -  test -           en:      2999
INFO - 06/10/22 12:37:10 - 0:00:06 - Parallel data      - valid -        de-en:      3000
INFO - 06/10/22 12:37:10 - 0:00:06 - Parallel data      -  test -        de-en:      2999

INFO - 06/10/22 12:37:13 - 0:00:09 - Reloading encoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
INFO - 06/10/22 12:37:15 - 0:00:12 - Reloading decoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
DEBUG - 06/10/22 12:37:17 - 0:00:14 - Encoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
DEBUG - 06/10/22 12:37:17 - 0:00:14 - Decoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
INFO - 06/10/22 12:37:17 - 0:00:14 - Number of parameters (encoder): 138023928
INFO - 06/10/22 12:37:17 - 0:00:14 - Number of parameters (decoder): 169524216
INFO - 06/10/22 12:37:20 - 0:00:16 - ============ Starting epoch 0 ... ============
INFO - 06/10/22 12:37:20 - 0:00:16 - Creating new training data iterator (bt,de) ...
INFO - 06/10/22 12:37:22 - 0:00:18 - iterator (bt,de) done
INFO - 06/10/22 13:14:30 - 0:00:00 - ============ Initialized logger ============
INFO - 06/10/22 13:14:30 - 0:00:00 - ae_steps: []
                                     asm: False
                                     attention_dropout: 0.1
                                     attention_setting: v1
                                     back_dataset: {}
                                     batch_size: 2
                                     beam_size: 1
                                     bmt_steps: []
                                     bptt: 128
                                     bt_src_langs: ['de', 'en']
                                     bt_steps: [('de', 'en', 'de'), ('en', 'de', 'en')]
                                     clip_grad_norm: 5
                                     clm_steps: []
                                     command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 2 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth' --exp_id "clm"
                                     context_size: 0
                                     cuda: False
                                     data_path: ./data/processed/de-en/
                                     debug_slurm: False
                                     debug_train: False
                                     dropout: 0.1
                                     dump_path: ./dumped/unsupMT_deen\clm
                                     early_stopping: False
                                     emb_dim: 1024
                                     encoder_only: False
                                     english_only: False
                                     epoch_size: 5
                                     eval_bleu: True
                                     eval_only: False
                                     exp_id: clm
                                     exp_name: unsupMT_deen
                                     fp16: False
                                     gelu_activation: True
                                     group_by_size: True
                                     id2lang: {0: 'de', 1: 'en'}
                                     is_master: True
                                     lambda_ae: 1
                                     lambda_bmt: 1
                                     lambda_bt: 1
                                     lambda_clm: 1
                                     lambda_mass: 1
                                     lambda_mlm: 1
                                     lambda_mt: 1
                                     lambda_pc: 1
                                     lambda_span: 10000
                                     lang2id: {'de': 0, 'en': 1}
                                     langs: ['de', 'en']
                                     length_penalty: 1
                                     lg_sampling_factor: -1
                                     lgs: de-en
                                     local_rank: 0
                                     mass_steps: []
                                     master_ip: None
                                     master_port: -1
                                     max_batch_size: 0
                                     max_epoch: 10
                                     max_len: 100
                                     max_vocab: -1
                                     min_count: 0
                                     min_len: 0
                                     mlm_steps: []
                                     mono_dataset: {'de': {'train': './data/processed/de-en/train.de.pth', 'valid': './data/processed/de-en/valid.de.pth', 'test': './data/processed/de-en/test.de.pth'}, 'en': {'train': './data/processed/de-en/train.en.pth', 'valid': './data/processed/de-en/valid.en.pth', 'test': './data/processed/de-en/test.en.pth'}}
                                     mt_steps: []
                                     multi_gpu: False
                                     multi_gpus: False
                                     n_dec_layers: 6
                                     n_gpu_per_node: 1
                                     n_heads: 8
                                     n_langs: 2
                                     n_layers: 6
                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001
                                     para_dataset: {('de', 'en'): {'valid': ('./data/processed/de-en/valid.de-en.de.pth', './data/processed/de-en/valid.de-en.en.pth'), 'test': ('./data/processed/de-en/test.de-en.de.pth', './data/processed/de-en/test.de-en.en.pth')}}
                                     pc_steps: []
                                     prefix: True
                                     reload_model: F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth
                                     sample_alpha: 0
                                     save_periodic: 0
                                     share_inout_emb: True
                                     sinusoidal_embeddings: False
                                     split_data: False
                                     stopping_criterion: _valid_de-en_mt_ppl,3
                                     temperature: None
                                     tokens_per_batch: 2000
                                     validation_metrics: _valid_de-en_mt_ppl
                                     word_blank: 0
                                     word_dropout: 0
                                     word_keep: 0.1
                                     word_mask: 0.8
                                     word_mask_keep_rand: 0.8,0.1,0.1
                                     word_mass: 0
                                     word_pred: 0.15
                                     word_rand: 0.1
                                     word_shuffle: 0
INFO - 06/10/22 13:14:30 - 0:00:00 - The experiment will be stored in ./dumped/unsupMT_deen\clm
                                     
INFO - 06/10/22 13:14:30 - 0:00:00 - Running command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --bt_steps 'de-en-de,en-de-en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 2 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu true --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth'

INFO - 06/10/22 13:14:30 - 0:00:00 - ============ Monolingual data (de)
INFO - 06/10/22 13:14:30 - 0:00:00 - Loading data from ./data/processed/de-en/train.de.pth ...
INFO - 06/10/22 13:14:30 - 0:00:00 - 108350453 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 13:14:31 - 0:00:01 - Removed 0 empty sentences.
INFO - 06/10/22 13:14:32 - 0:00:02 - Removed 6354 too long sentences.

INFO - 06/10/22 13:14:32 - 0:00:02 - Loading data from ./data/processed/de-en/valid.de.pth ...
INFO - 06/10/22 13:14:32 - 0:00:02 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.

INFO - 06/10/22 13:14:32 - 0:00:02 - Loading data from ./data/processed/de-en/test.de.pth ...
INFO - 06/10/22 13:14:32 - 0:00:02 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/10/22 13:14:32 - 0:00:02 - ============ Monolingual data (en)
INFO - 06/10/22 13:14:32 - 0:00:02 - Loading data from ./data/processed/de-en/train.en.pth ...
INFO - 06/10/22 13:14:33 - 0:00:03 - 129883113 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 13:14:34 - 0:00:04 - Removed 0 empty sentences.
INFO - 06/10/22 13:14:34 - 0:00:04 - Removed 13059 too long sentences.

INFO - 06/10/22 13:14:34 - 0:00:04 - Loading data from ./data/processed/de-en/valid.en.pth ...
INFO - 06/10/22 13:14:34 - 0:00:04 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.

INFO - 06/10/22 13:14:34 - 0:00:04 - Loading data from ./data/processed/de-en/test.en.pth ...
INFO - 06/10/22 13:14:34 - 0:00:04 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.


INFO - 06/10/22 13:14:35 - 0:00:05 - ============ Parallel data (de-en)
INFO - 06/10/22 13:14:35 - 0:00:05 - Loading data from ./data/processed/de-en/valid.de-en.de.pth ...
INFO - 06/10/22 13:14:35 - 0:00:05 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.
INFO - 06/10/22 13:14:35 - 0:00:05 - Loading data from ./data/processed/de-en/valid.de-en.en.pth ...
INFO - 06/10/22 13:14:35 - 0:00:05 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.
INFO - 06/10/22 13:14:35 - 0:00:05 - Removed 0 empty sentences.

INFO - 06/10/22 13:14:35 - 0:00:05 - Loading data from ./data/processed/de-en/test.de-en.de.pth ...
INFO - 06/10/22 13:14:35 - 0:00:05 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 13:14:35 - 0:00:05 - Loading data from ./data/processed/de-en/test.de-en.en.pth ...
INFO - 06/10/22 13:14:35 - 0:00:05 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.
INFO - 06/10/22 13:14:36 - 0:00:06 - Removed 0 empty sentences.


INFO - 06/10/22 13:14:36 - 0:00:06 - ============ Data summary
INFO - 06/10/22 13:14:36 - 0:00:06 - Monolingual data   - train -           de:   5000000
INFO - 06/10/22 13:14:36 - 0:00:06 - Monolingual data   - valid -           de:      3000
INFO - 06/10/22 13:14:36 - 0:00:06 - Monolingual data   -  test -           de:      2999
INFO - 06/10/22 13:14:36 - 0:00:06 - Monolingual data   - train -           en:   5000000
INFO - 06/10/22 13:14:36 - 0:00:06 - Monolingual data   - valid -           en:      3000
INFO - 06/10/22 13:14:36 - 0:00:06 - Monolingual data   -  test -           en:      2999
INFO - 06/10/22 13:14:36 - 0:00:06 - Parallel data      - valid -        de-en:      3000
INFO - 06/10/22 13:14:36 - 0:00:06 - Parallel data      -  test -        de-en:      2999

INFO - 06/10/22 13:14:39 - 0:00:09 - Reloading encoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
INFO - 06/10/22 13:14:41 - 0:00:11 - Reloading decoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
DEBUG - 06/10/22 13:14:43 - 0:00:13 - Encoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
DEBUG - 06/10/22 13:14:43 - 0:00:13 - Decoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
INFO - 06/10/22 13:14:43 - 0:00:13 - Number of parameters (encoder): 138023928
INFO - 06/10/22 13:14:43 - 0:00:13 - Number of parameters (decoder): 169524216
INFO - 06/10/22 13:14:47 - 0:00:17 - ============ Starting epoch 0 ... ============
INFO - 06/10/22 13:14:47 - 0:00:17 - Creating new training data iterator (bt,en) ...
INFO - 06/10/22 13:14:48 - 0:00:18 - iterator (bt,en) done
INFO - 06/14/22 10:27:31 - 0:00:00 - ============ Initialized logger ============
INFO - 06/14/22 10:27:31 - 0:00:00 - ae_steps: []
                                     asm: False
                                     attention_dropout: 0.1
                                     attention_setting: v1
                                     back_dataset: {}
                                     batch_size: 2
                                     beam_size: 1
                                     bmt_steps: []
                                     bptt: 128
                                     bt_src_langs: []
                                     bt_steps: []
                                     clip_grad_norm: 5
                                     clm_steps: [('de', None), ('en', None)]
                                     command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 2 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu false --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth' --exp_id "clm"
                                     context_size: 0
                                     cuda: False
                                     data_path: ./data/processed/de-en/
                                     debug_slurm: False
                                     debug_train: False
                                     dropout: 0.1
                                     dump_path: ./dumped/unsupMT_deen\clm
                                     early_stopping: False
                                     emb_dim: 1024
                                     encoder_only: False
                                     english_only: False
                                     epoch_size: 5
                                     eval_bleu: False
                                     eval_only: False
                                     exp_id: clm
                                     exp_name: unsupMT_deen
                                     fp16: False
                                     gelu_activation: True
                                     group_by_size: True
                                     id2lang: {0: 'de', 1: 'en'}
                                     is_master: True
                                     lambda_ae: 1
                                     lambda_bmt: 1
                                     lambda_bt: 1
                                     lambda_clm: 1
                                     lambda_mass: 1
                                     lambda_mlm: 1
                                     lambda_mt: 1
                                     lambda_pc: 1
                                     lambda_span: 10000
                                     lang2id: {'de': 0, 'en': 1}
                                     langs: ['de', 'en']
                                     length_penalty: 1
                                     lg_sampling_factor: -1
                                     lgs: de-en
                                     local_rank: 0
                                     mass_steps: []
                                     master_ip: None
                                     master_port: -1
                                     max_batch_size: 0
                                     max_epoch: 10
                                     max_len: 100
                                     max_vocab: -1
                                     min_count: 0
                                     min_len: 0
                                     mlm_steps: []
                                     mono_dataset: {'de': {'train': './data/processed/de-en/train.de.pth', 'valid': './data/processed/de-en/valid.de.pth', 'test': './data/processed/de-en/test.de.pth'}, 'en': {'train': './data/processed/de-en/train.en.pth', 'valid': './data/processed/de-en/valid.en.pth', 'test': './data/processed/de-en/test.en.pth'}}
                                     mt_steps: []
                                     multi_gpu: False
                                     multi_gpus: False
                                     n_dec_layers: 6
                                     n_gpu_per_node: 1
                                     n_heads: 8
                                     n_langs: 2
                                     n_layers: 6
                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001
                                     para_dataset: {}
                                     pc_steps: []
                                     prefix: True
                                     reload_model: F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth
                                     sample_alpha: 0
                                     save_periodic: 0
                                     share_inout_emb: True
                                     sinusoidal_embeddings: False
                                     split_data: False
                                     stopping_criterion: _valid_de-en_mt_ppl,3
                                     temperature: None
                                     tokens_per_batch: 2000
                                     validation_metrics: _valid_de-en_mt_ppl
                                     word_blank: 0
                                     word_dropout: 0
                                     word_keep: 0.1
                                     word_mask: 0.8
                                     word_mask_keep_rand: 0.8,0.1,0.1
                                     word_mass: 0
                                     word_pred: 0.15
                                     word_rand: 0.1
                                     word_shuffle: 0
INFO - 06/14/22 10:27:31 - 0:00:00 - The experiment will be stored in ./dumped/unsupMT_deen\clm
                                     
INFO - 06/14/22 10:27:31 - 0:00:00 - Running command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 2 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu false --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth'

INFO - 06/14/22 10:27:31 - 0:00:00 - ============ Monolingual data (de)
INFO - 06/14/22 10:27:31 - 0:00:00 - Loading data from ./data/processed/de-en/train.de.pth ...
INFO - 06/14/22 10:27:32 - 0:00:01 - 108350453 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/14/22 10:27:33 - 0:00:02 - Loading data from ./data/processed/de-en/valid.de.pth ...
INFO - 06/14/22 10:27:33 - 0:00:02 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.

INFO - 06/14/22 10:27:33 - 0:00:02 - Loading data from ./data/processed/de-en/test.de.pth ...
INFO - 06/14/22 10:27:33 - 0:00:02 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/14/22 10:27:34 - 0:00:03 - ============ Monolingual data (en)
INFO - 06/14/22 10:27:34 - 0:00:03 - Loading data from ./data/processed/de-en/train.en.pth ...
INFO - 06/14/22 10:27:34 - 0:00:03 - 129883113 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/14/22 10:27:36 - 0:00:05 - Loading data from ./data/processed/de-en/valid.en.pth ...
INFO - 06/14/22 10:27:36 - 0:00:05 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.

INFO - 06/14/22 10:27:36 - 0:00:05 - Loading data from ./data/processed/de-en/test.en.pth ...
INFO - 06/14/22 10:27:36 - 0:00:05 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.



INFO - 06/14/22 10:27:37 - 0:00:06 - ============ Data summary
INFO - 06/14/22 10:27:37 - 0:00:06 - Monolingual data   - train -           de:   5000000
INFO - 06/14/22 10:27:37 - 0:00:06 - Monolingual data   - valid -           de:      3000
INFO - 06/14/22 10:27:37 - 0:00:06 - Monolingual data   -  test -           de:      2999
INFO - 06/14/22 10:27:37 - 0:00:06 - Monolingual data   - train -           en:   5000000
INFO - 06/14/22 10:27:37 - 0:00:06 - Monolingual data   - valid -           en:      3000
INFO - 06/14/22 10:27:37 - 0:00:06 - Monolingual data   -  test -           en:      2999

INFO - 06/14/22 10:27:42 - 0:00:11 - Reloading encoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
INFO - 06/14/22 10:27:45 - 0:00:13 - Reloading decoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
DEBUG - 06/14/22 10:27:47 - 0:00:16 - Encoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
DEBUG - 06/14/22 10:27:47 - 0:00:16 - Decoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
INFO - 06/14/22 10:27:47 - 0:00:16 - Number of parameters (encoder): 138023928
INFO - 06/14/22 10:27:47 - 0:00:16 - Number of parameters (decoder): 169524216
INFO - 06/14/22 10:27:47 - 0:00:16 - ============ Starting epoch 0 ... ============
INFO - 06/14/22 10:27:47 - 0:00:16 - Creating new training data iterator (causal,en) ...
INFO - 06/14/22 10:27:47 - 0:00:16 - iterator (causal,en) done
INFO - 06/21/22 15:42:42 - 0:00:00 - ============ Initialized logger ============
INFO - 06/21/22 15:42:42 - 0:00:00 - ae_steps: []
                                     asm: False
                                     attention_dropout: 0.1
                                     attention_setting: v1
                                     back_dataset: {}
                                     batch_size: 2
                                     beam_size: 1
                                     bmt_steps: []
                                     bptt: 128
                                     bt_src_langs: []
                                     bt_steps: []
                                     clip_grad_norm: 5
                                     clm_steps: [('de', None), ('en', None)]
                                     command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 2 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu false --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth' --exp_id "clm"
                                     context_size: 0
                                     cuda: False
                                     data_path: ./data/processed/de-en/
                                     debug_slurm: False
                                     debug_train: False
                                     dropout: 0.1
                                     dump_path: ./dumped/unsupMT_deen\clm
                                     early_stopping: False
                                     emb_dim: 1024
                                     encoder_only: False
                                     english_only: False
                                     epoch_size: 5
                                     eval_bleu: False
                                     eval_only: False
                                     exp_id: clm
                                     exp_name: unsupMT_deen
                                     fp16: False
                                     gelu_activation: True
                                     group_by_size: True
                                     id2lang: {0: 'de', 1: 'en'}
                                     is_master: True
                                     lambda_ae: 1
                                     lambda_bmt: 1
                                     lambda_bt: 1
                                     lambda_clm: 1
                                     lambda_mass: 1
                                     lambda_mlm: 1
                                     lambda_mt: 1
                                     lambda_pc: 1
                                     lambda_span: 10000
                                     lang2id: {'de': 0, 'en': 1}
                                     langs: ['de', 'en']
                                     length_penalty: 1
                                     lg_sampling_factor: -1
                                     lgs: de-en
                                     local_rank: 0
                                     mass_steps: []
                                     master_ip: None
                                     master_port: -1
                                     max_batch_size: 0
                                     max_epoch: 10
                                     max_len: 100
                                     max_vocab: -1
                                     min_count: 0
                                     min_len: 0
                                     mlm_steps: []
                                     mono_dataset: {'de': {'train': './data/processed/de-en/train.de.pth', 'valid': './data/processed/de-en/valid.de.pth', 'test': './data/processed/de-en/test.de.pth'}, 'en': {'train': './data/processed/de-en/train.en.pth', 'valid': './data/processed/de-en/valid.en.pth', 'test': './data/processed/de-en/test.en.pth'}}
                                     mt_steps: []
                                     multi_gpu: False
                                     multi_gpus: False
                                     n_dec_layers: 6
                                     n_gpu_per_node: 1
                                     n_heads: 8
                                     n_langs: 2
                                     n_layers: 6
                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001
                                     para_dataset: {}
                                     pc_steps: []
                                     prefix: True
                                     reload_model: F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth
                                     sample_alpha: 0
                                     save_periodic: 0
                                     share_inout_emb: True
                                     sinusoidal_embeddings: False
                                     split_data: False
                                     stopping_criterion: _valid_de-en_mt_ppl,3
                                     temperature: None
                                     tokens_per_batch: 2000
                                     validation_metrics: _valid_de-en_mt_ppl
                                     word_blank: 0
                                     word_dropout: 0
                                     word_keep: 0.1
                                     word_mask: 0.8
                                     word_mask_keep_rand: 0.8,0.1,0.1
                                     word_mass: 0
                                     word_pred: 0.15
                                     word_rand: 0.1
                                     word_shuffle: 0
INFO - 06/21/22 15:42:42 - 0:00:00 - The experiment will be stored in ./dumped/unsupMT_deen\clm
                                     
INFO - 06/21/22 15:42:42 - 0:00:00 - Running command: python train.py --cuda False --prefix True --exp_name unsupMT_deen --exp_id clm --data_path './data/processed/de-en/' --lgs 'de-en' --clm_steps 'de,en' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 2 --bptt 128 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 5 --max_epoch 10 --eval_bleu false --validation_metrics '_valid_de-en_mt_ppl' --stopping_criterion '_valid_de-en_mt_ppl,3' --reload_model 'F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth,F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth'

INFO - 06/21/22 15:42:42 - 0:00:00 - ============ Monolingual data (de)
INFO - 06/21/22 15:42:42 - 0:00:00 - Loading data from ./data/processed/de-en/train.de.pth ...
INFO - 06/21/22 15:42:43 - 0:00:00 - 108350453 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/21/22 15:42:43 - 0:00:01 - Loading data from ./data/processed/de-en/valid.de.pth ...
INFO - 06/21/22 15:42:43 - 0:00:01 - 74899 words (60408 unique) in 3000 sentences. 4 unknown words (3 unique) covering 0.01% of the data.

INFO - 06/21/22 15:42:43 - 0:00:01 - Loading data from ./data/processed/de-en/test.de.pth ...
INFO - 06/21/22 15:42:44 - 0:00:01 - 73662 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/21/22 15:42:44 - 0:00:01 - ============ Monolingual data (en)
INFO - 06/21/22 15:42:44 - 0:00:01 - Loading data from ./data/processed/de-en/train.en.pth ...
INFO - 06/21/22 15:42:44 - 0:00:02 - 129883113 words (60408 unique) in 5000000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.

INFO - 06/21/22 15:42:45 - 0:00:03 - Loading data from ./data/processed/de-en/valid.en.pth ...
INFO - 06/21/22 15:42:45 - 0:00:03 - 70574 words (60408 unique) in 3000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.

INFO - 06/21/22 15:42:45 - 0:00:03 - Loading data from ./data/processed/de-en/test.en.pth ...
INFO - 06/21/22 15:42:45 - 0:00:03 - 70156 words (60408 unique) in 2999 sentences. 0 unknown words (0 unique) covering 0.00% of the data.



INFO - 06/21/22 15:42:46 - 0:00:03 - ============ Data summary
INFO - 06/21/22 15:42:46 - 0:00:03 - Monolingual data   - train -           de:   5000000
INFO - 06/21/22 15:42:46 - 0:00:03 - Monolingual data   - valid -           de:      3000
INFO - 06/21/22 15:42:46 - 0:00:03 - Monolingual data   -  test -           de:      2999
INFO - 06/21/22 15:42:46 - 0:00:03 - Monolingual data   - train -           en:   5000000
INFO - 06/21/22 15:42:46 - 0:00:03 - Monolingual data   - valid -           en:      3000
INFO - 06/21/22 15:42:46 - 0:00:03 - Monolingual data   -  test -           en:      2999

INFO - 06/21/22 15:42:49 - 0:00:07 - Reloading encoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
INFO - 06/21/22 15:42:51 - 0:00:09 - Reloading decoder from F:\\project\\UNMT\\LM\\CBD\\mass_ft_ende_1024.pth ...
DEBUG - 06/21/22 15:42:53 - 0:00:10 - Encoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
DEBUG - 06/21/22 15:42:53 - 0:00:10 - Decoder: TransformerModel(
                                        (position_embeddings): Embedding(512, 1024)
                                        (lang_embeddings): Embedding(2, 1024)
                                        (embeddings): Embedding(60408, 1024, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=1024, out_features=4096, bias=True)
                                            (lin2): Linear(in_features=4096, out_features=1024, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
                                            (out_lin): ModuleList(
                                              (0): Linear(in_features=1024, out_features=1024, bias=True)
                                              (1): Linear(in_features=1024, out_features=1024, bias=True)
                                            )
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=1024, out_features=60408, bias=True)
                                        )
                                      )
INFO - 06/21/22 15:42:53 - 0:00:10 - Number of parameters (encoder): 138023928
INFO - 06/21/22 15:42:53 - 0:00:10 - Number of parameters (decoder): 169524216
INFO - 06/21/22 15:42:54 - 0:00:12 - ============ Starting epoch 0 ... ============
INFO - 06/21/22 15:42:54 - 0:00:12 - Creating new training data iterator (causal,en) ...
INFO - 06/21/22 15:42:54 - 0:00:12 - iterator (causal,en) done
